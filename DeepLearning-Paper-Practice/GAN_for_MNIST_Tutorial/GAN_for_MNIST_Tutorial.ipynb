{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (NIPS 2014)\n",
    "<br>\n",
    "<br>\n",
    "<b>References</b>\n",
    "<br>\n",
    "This notebook is created by learning from the following notebooks:\n",
    "\n",
    "- https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/GAN_for_MNIST_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:02:28.703471Z",
     "start_time": "2021-05-26T10:02:28.700735Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 생성자(Generator) 및 판별자(Discriminator) 모델 정의"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "\n",
    "# 생성자(Generator) 클래스 정의\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # 하나의 블록(block) 정의\n",
    "        def block(input_dim, output_dim, normalize=True):\n",
    "            layers = [nn.Linear(input_dim, output_dim)]\n",
    "            # 배치 정규화(batch normalization) 수행(차원 동일)\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(output_dim, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        # 생성자 모델은 연속적인 여러 개의 블록을 가짐\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, 1*28*28),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 판별자(Discriminator) 클래스 정의\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1 * 28 * 28, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        flattened = img.view(img.size(0), -1)\n",
    "        output = self.model(flattened)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 학습 데이터셋 불러오기\n",
    "- 학습을 위해 MNIST 데이터셋을 불러온다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./dataset\", train=True, download=True, transform=transforms_train)\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 학습 및 샘플링\n",
    "- 학습을 위해 생성자의 판별자 모델 초기화\n",
    "- 적절한 하이퍼 파라미터를 설정"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# 생성자(Generator)와 판별자(Discriminator) 초기화\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.cuda()\n",
    "discriminator.cuda()\n",
    "\n",
    "# 손실 함수(loss function)\n",
    "adversarial_loss = nn.BCELoss()\n",
    "adversarial_loss.cuda()\n",
    "\n",
    "# 학습률(learning rate) 설정\n",
    "lr = 0.0002\n",
    "\n",
    "# 생성자와 판별자를 위한 최적화 함수\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr,\n",
    "                               betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr,\n",
    "                               betas=(0.5, 0.999))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 모델을 학습하면서 주기적으로 샘플링 하여 결과를 확인 할 수 있습니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [D loss: 0.542029] [G loss: 0.782794] [Elapsed time: 1.76s]\n",
      "[Epoch 1/200] [D loss: 0.483996] [G loss: 0.920153] [Elapsed time: 3.48s]\n",
      "[Epoch 2/200] [D loss: 0.546040] [G loss: 1.222871] [Elapsed time: 5.25s]\n",
      "[Epoch 3/200] [D loss: 0.366962] [G loss: 1.638766] [Elapsed time: 6.91s]\n",
      "[Epoch 4/200] [D loss: 0.576263] [G loss: 0.487400] [Elapsed time: 8.72s]\n",
      "[Epoch 5/200] [D loss: 0.490790] [G loss: 1.669144] [Elapsed time: 10.44s]\n",
      "[Epoch 6/200] [D loss: 0.878131] [G loss: 3.614223] [Elapsed time: 12.03s]\n",
      "[Epoch 7/200] [D loss: 0.434505] [G loss: 2.760345] [Elapsed time: 13.69s]\n",
      "[Epoch 8/200] [D loss: 0.574028] [G loss: 3.446070] [Elapsed time: 15.45s]\n",
      "[Epoch 9/200] [D loss: 0.269721] [G loss: 1.253381] [Elapsed time: 17.26s]\n",
      "[Epoch 10/200] [D loss: 0.413095] [G loss: 0.735877] [Elapsed time: 18.90s]\n",
      "[Epoch 11/200] [D loss: 0.301562] [G loss: 1.641016] [Elapsed time: 20.69s]\n",
      "[Epoch 12/200] [D loss: 0.808327] [G loss: 4.553882] [Elapsed time: 22.35s]\n",
      "[Epoch 13/200] [D loss: 0.220229] [G loss: 1.817601] [Elapsed time: 24.06s]\n",
      "[Epoch 14/200] [D loss: 0.357062] [G loss: 2.925793] [Elapsed time: 25.78s]\n",
      "[Epoch 15/200] [D loss: 0.264775] [G loss: 1.262225] [Elapsed time: 27.46s]\n",
      "[Epoch 16/200] [D loss: 0.255495] [G loss: 2.489917] [Elapsed time: 29.13s]\n",
      "[Epoch 17/200] [D loss: 0.296383] [G loss: 3.744852] [Elapsed time: 30.86s]\n",
      "[Epoch 18/200] [D loss: 0.277050] [G loss: 1.112361] [Elapsed time: 32.53s]\n",
      "[Epoch 19/200] [D loss: 0.282365] [G loss: 1.720140] [Elapsed time: 34.22s]\n",
      "[Epoch 20/200] [D loss: 0.186589] [G loss: 2.299919] [Elapsed time: 35.86s]\n",
      "[Epoch 21/200] [D loss: 0.164391] [G loss: 2.130789] [Elapsed time: 37.59s]\n",
      "[Epoch 22/200] [D loss: 0.261398] [G loss: 2.653491] [Elapsed time: 39.27s]\n",
      "[Epoch 23/200] [D loss: 0.115537] [G loss: 2.632499] [Elapsed time: 40.97s]\n",
      "[Epoch 24/200] [D loss: 0.128209] [G loss: 2.363629] [Elapsed time: 42.73s]\n",
      "[Epoch 25/200] [D loss: 0.190300] [G loss: 1.675275] [Elapsed time: 44.43s]\n",
      "[Epoch 26/200] [D loss: 0.279310] [G loss: 1.524361] [Elapsed time: 46.13s]\n",
      "[Epoch 27/200] [D loss: 0.167907] [G loss: 1.970078] [Elapsed time: 47.86s]\n",
      "[Epoch 28/200] [D loss: 0.246055] [G loss: 1.462473] [Elapsed time: 49.57s]\n",
      "[Epoch 29/200] [D loss: 0.201172] [G loss: 1.483109] [Elapsed time: 51.26s]\n",
      "[Epoch 30/200] [D loss: 0.257727] [G loss: 1.457126] [Elapsed time: 52.92s]\n",
      "[Epoch 31/200] [D loss: 0.202874] [G loss: 1.815922] [Elapsed time: 54.62s]\n",
      "[Epoch 32/200] [D loss: 0.250434] [G loss: 2.008863] [Elapsed time: 56.36s]\n",
      "[Epoch 33/200] [D loss: 0.279085] [G loss: 1.162458] [Elapsed time: 58.01s]\n",
      "[Epoch 34/200] [D loss: 0.190608] [G loss: 1.735262] [Elapsed time: 59.80s]\n",
      "[Epoch 35/200] [D loss: 0.470306] [G loss: 0.700561] [Elapsed time: 61.57s]\n",
      "[Epoch 36/200] [D loss: 0.241937] [G loss: 2.395857] [Elapsed time: 63.33s]\n",
      "[Epoch 37/200] [D loss: 0.170347] [G loss: 2.248475] [Elapsed time: 65.03s]\n",
      "[Epoch 38/200] [D loss: 0.161458] [G loss: 1.950970] [Elapsed time: 66.71s]\n",
      "[Epoch 39/200] [D loss: 0.247683] [G loss: 3.439292] [Elapsed time: 68.46s]\n",
      "[Epoch 40/200] [D loss: 0.332257] [G loss: 1.021985] [Elapsed time: 70.16s]\n",
      "[Epoch 41/200] [D loss: 0.247218] [G loss: 3.178174] [Elapsed time: 71.84s]\n",
      "[Epoch 42/200] [D loss: 0.583742] [G loss: 4.966096] [Elapsed time: 73.53s]\n",
      "[Epoch 43/200] [D loss: 0.322472] [G loss: 1.161251] [Elapsed time: 75.23s]\n",
      "[Epoch 44/200] [D loss: 0.236471] [G loss: 2.974072] [Elapsed time: 76.86s]\n",
      "[Epoch 45/200] [D loss: 0.198875] [G loss: 1.578738] [Elapsed time: 78.44s]\n",
      "[Epoch 46/200] [D loss: 0.321247] [G loss: 1.116529] [Elapsed time: 80.07s]\n",
      "[Epoch 47/200] [D loss: 0.210194] [G loss: 1.905532] [Elapsed time: 81.71s]\n",
      "[Epoch 48/200] [D loss: 0.301389] [G loss: 1.255906] [Elapsed time: 83.32s]\n",
      "[Epoch 49/200] [D loss: 0.439300] [G loss: 1.052922] [Elapsed time: 85.09s]\n",
      "[Epoch 50/200] [D loss: 0.184376] [G loss: 2.119452] [Elapsed time: 86.70s]\n",
      "[Epoch 51/200] [D loss: 0.277884] [G loss: 1.938441] [Elapsed time: 88.40s]\n",
      "[Epoch 52/200] [D loss: 0.398186] [G loss: 3.913745] [Elapsed time: 90.10s]\n",
      "[Epoch 53/200] [D loss: 0.228510] [G loss: 1.313550] [Elapsed time: 91.82s]\n",
      "[Epoch 54/200] [D loss: 0.267057] [G loss: 1.508820] [Elapsed time: 93.53s]\n",
      "[Epoch 55/200] [D loss: 0.268668] [G loss: 2.261429] [Elapsed time: 95.21s]\n",
      "[Epoch 56/200] [D loss: 0.196164] [G loss: 3.391532] [Elapsed time: 96.83s]\n",
      "[Epoch 57/200] [D loss: 0.621905] [G loss: 3.848887] [Elapsed time: 98.52s]\n",
      "[Epoch 58/200] [D loss: 0.185206] [G loss: 2.627591] [Elapsed time: 100.14s]\n",
      "[Epoch 59/200] [D loss: 0.317766] [G loss: 3.919710] [Elapsed time: 101.78s]\n",
      "[Epoch 60/200] [D loss: 0.177142] [G loss: 1.923420] [Elapsed time: 103.44s]\n",
      "[Epoch 61/200] [D loss: 0.460303] [G loss: 3.359352] [Elapsed time: 105.16s]\n",
      "[Epoch 62/200] [D loss: 0.282890] [G loss: 2.536056] [Elapsed time: 106.83s]\n",
      "[Epoch 63/200] [D loss: 0.351793] [G loss: 1.422623] [Elapsed time: 108.48s]\n",
      "[Epoch 64/200] [D loss: 0.832725] [G loss: 5.489104] [Elapsed time: 110.12s]\n",
      "[Epoch 65/200] [D loss: 0.277670] [G loss: 1.577129] [Elapsed time: 111.80s]\n",
      "[Epoch 66/200] [D loss: 0.336857] [G loss: 1.068402] [Elapsed time: 113.54s]\n",
      "[Epoch 67/200] [D loss: 0.262897] [G loss: 1.723815] [Elapsed time: 115.29s]\n",
      "[Epoch 68/200] [D loss: 0.193869] [G loss: 2.055580] [Elapsed time: 116.96s]\n",
      "[Epoch 69/200] [D loss: 0.507847] [G loss: 4.843295] [Elapsed time: 118.62s]\n",
      "[Epoch 70/200] [D loss: 0.313302] [G loss: 1.385567] [Elapsed time: 120.35s]\n",
      "[Epoch 71/200] [D loss: 0.280936] [G loss: 1.725308] [Elapsed time: 122.14s]\n",
      "[Epoch 72/200] [D loss: 0.185763] [G loss: 2.088987] [Elapsed time: 123.84s]\n",
      "[Epoch 73/200] [D loss: 0.348842] [G loss: 4.896725] [Elapsed time: 125.53s]\n",
      "[Epoch 74/200] [D loss: 0.219507] [G loss: 2.378501] [Elapsed time: 127.20s]\n",
      "[Epoch 75/200] [D loss: 0.232541] [G loss: 2.204533] [Elapsed time: 128.86s]\n",
      "[Epoch 76/200] [D loss: 0.192198] [G loss: 1.837487] [Elapsed time: 130.50s]\n",
      "[Epoch 77/200] [D loss: 0.303106] [G loss: 2.794460] [Elapsed time: 132.17s]\n",
      "[Epoch 78/200] [D loss: 0.433636] [G loss: 0.819195] [Elapsed time: 133.92s]\n",
      "[Epoch 79/200] [D loss: 0.271949] [G loss: 1.301208] [Elapsed time: 135.57s]\n",
      "[Epoch 80/200] [D loss: 0.489823] [G loss: 0.826244] [Elapsed time: 137.23s]\n",
      "[Epoch 81/200] [D loss: 0.307913] [G loss: 2.076090] [Elapsed time: 138.83s]\n",
      "[Epoch 82/200] [D loss: 0.320977] [G loss: 2.975718] [Elapsed time: 140.52s]\n",
      "[Epoch 83/200] [D loss: 0.308148] [G loss: 2.209182] [Elapsed time: 142.12s]\n",
      "[Epoch 84/200] [D loss: 0.283072] [G loss: 1.371263] [Elapsed time: 143.75s]\n",
      "[Epoch 85/200] [D loss: 0.364798] [G loss: 1.154472] [Elapsed time: 145.48s]\n",
      "[Epoch 86/200] [D loss: 0.368272] [G loss: 1.888980] [Elapsed time: 147.18s]\n",
      "[Epoch 87/200] [D loss: 0.312353] [G loss: 1.169155] [Elapsed time: 148.85s]\n",
      "[Epoch 88/200] [D loss: 0.270627] [G loss: 1.890848] [Elapsed time: 150.48s]\n",
      "[Epoch 89/200] [D loss: 0.290950] [G loss: 2.804919] [Elapsed time: 152.19s]\n",
      "[Epoch 90/200] [D loss: 0.408642] [G loss: 1.869177] [Elapsed time: 153.93s]\n",
      "[Epoch 91/200] [D loss: 0.509959] [G loss: 0.927358] [Elapsed time: 155.67s]\n",
      "[Epoch 92/200] [D loss: 0.323245] [G loss: 3.165766] [Elapsed time: 157.35s]\n",
      "[Epoch 93/200] [D loss: 0.413934] [G loss: 2.393776] [Elapsed time: 158.98s]\n",
      "[Epoch 94/200] [D loss: 0.369421] [G loss: 1.019908] [Elapsed time: 160.65s]\n",
      "[Epoch 95/200] [D loss: 0.299672] [G loss: 1.217701] [Elapsed time: 162.27s]\n",
      "[Epoch 96/200] [D loss: 0.387515] [G loss: 1.024393] [Elapsed time: 163.82s]\n",
      "[Epoch 97/200] [D loss: 0.368679] [G loss: 1.112924] [Elapsed time: 165.61s]\n",
      "[Epoch 98/200] [D loss: 0.340523] [G loss: 1.071228] [Elapsed time: 167.24s]\n",
      "[Epoch 99/200] [D loss: 0.300948] [G loss: 1.544840] [Elapsed time: 168.85s]\n",
      "[Epoch 100/200] [D loss: 0.359541] [G loss: 1.208551] [Elapsed time: 170.54s]\n",
      "[Epoch 101/200] [D loss: 0.267160] [G loss: 2.312664] [Elapsed time: 172.26s]\n",
      "[Epoch 102/200] [D loss: 0.328274] [G loss: 2.939742] [Elapsed time: 173.97s]\n",
      "[Epoch 103/200] [D loss: 0.303582] [G loss: 2.747334] [Elapsed time: 175.73s]\n",
      "[Epoch 104/200] [D loss: 0.398528] [G loss: 1.005172] [Elapsed time: 177.47s]\n",
      "[Epoch 105/200] [D loss: 0.282546] [G loss: 1.929134] [Elapsed time: 179.10s]\n",
      "[Epoch 106/200] [D loss: 0.256272] [G loss: 1.548488] [Elapsed time: 180.79s]\n",
      "[Epoch 107/200] [D loss: 0.262976] [G loss: 2.364267] [Elapsed time: 182.46s]\n",
      "[Epoch 108/200] [D loss: 0.409600] [G loss: 3.688717] [Elapsed time: 184.13s]\n",
      "[Epoch 109/200] [D loss: 0.352613] [G loss: 1.475626] [Elapsed time: 185.90s]\n",
      "[Epoch 110/200] [D loss: 0.375949] [G loss: 1.108189] [Elapsed time: 187.52s]\n",
      "[Epoch 111/200] [D loss: 0.278281] [G loss: 1.755196] [Elapsed time: 189.31s]\n",
      "[Epoch 112/200] [D loss: 0.295700] [G loss: 1.641028] [Elapsed time: 191.04s]\n",
      "[Epoch 113/200] [D loss: 0.256368] [G loss: 1.753567] [Elapsed time: 192.74s]\n",
      "[Epoch 114/200] [D loss: 0.286924] [G loss: 1.624233] [Elapsed time: 194.44s]\n",
      "[Epoch 115/200] [D loss: 0.393449] [G loss: 1.412715] [Elapsed time: 196.07s]\n",
      "[Epoch 116/200] [D loss: 0.378026] [G loss: 2.755767] [Elapsed time: 197.73s]\n",
      "[Epoch 117/200] [D loss: 0.387994] [G loss: 1.142146] [Elapsed time: 199.37s]\n",
      "[Epoch 118/200] [D loss: 0.301907] [G loss: 2.514403] [Elapsed time: 200.95s]\n",
      "[Epoch 119/200] [D loss: 0.292384] [G loss: 1.383207] [Elapsed time: 202.73s]\n",
      "[Epoch 120/200] [D loss: 0.304146] [G loss: 1.598457] [Elapsed time: 204.47s]\n",
      "[Epoch 121/200] [D loss: 0.255318] [G loss: 1.708903] [Elapsed time: 206.15s]\n",
      "[Epoch 122/200] [D loss: 0.242396] [G loss: 1.658355] [Elapsed time: 207.80s]\n",
      "[Epoch 123/200] [D loss: 0.434719] [G loss: 0.945752] [Elapsed time: 209.44s]\n",
      "[Epoch 124/200] [D loss: 0.240562] [G loss: 1.738209] [Elapsed time: 211.19s]\n",
      "[Epoch 125/200] [D loss: 0.243903] [G loss: 1.754727] [Elapsed time: 212.99s]\n",
      "[Epoch 126/200] [D loss: 0.282588] [G loss: 2.007067] [Elapsed time: 214.68s]\n",
      "[Epoch 127/200] [D loss: 0.311258] [G loss: 1.449052] [Elapsed time: 216.33s]\n",
      "[Epoch 128/200] [D loss: 0.378463] [G loss: 1.426200] [Elapsed time: 217.93s]\n",
      "[Epoch 129/200] [D loss: 0.567925] [G loss: 3.406538] [Elapsed time: 219.63s]\n",
      "[Epoch 130/200] [D loss: 0.397196] [G loss: 2.592548] [Elapsed time: 221.25s]\n",
      "[Epoch 131/200] [D loss: 0.323155] [G loss: 1.693205] [Elapsed time: 222.86s]\n",
      "[Epoch 132/200] [D loss: 0.398007] [G loss: 3.737726] [Elapsed time: 224.57s]\n",
      "[Epoch 133/200] [D loss: 0.411139] [G loss: 1.492823] [Elapsed time: 226.22s]\n",
      "[Epoch 134/200] [D loss: 0.553432] [G loss: 4.170094] [Elapsed time: 227.86s]\n",
      "[Epoch 135/200] [D loss: 0.405237] [G loss: 1.143598] [Elapsed time: 229.51s]\n",
      "[Epoch 136/200] [D loss: 0.327275] [G loss: 1.324484] [Elapsed time: 231.23s]\n",
      "[Epoch 137/200] [D loss: 0.274599] [G loss: 1.726784] [Elapsed time: 232.95s]\n",
      "[Epoch 138/200] [D loss: 0.301366] [G loss: 2.485254] [Elapsed time: 234.63s]\n",
      "[Epoch 139/200] [D loss: 0.246346] [G loss: 2.069226] [Elapsed time: 236.28s]\n",
      "[Epoch 140/200] [D loss: 0.262219] [G loss: 2.056676] [Elapsed time: 237.94s]\n",
      "[Epoch 141/200] [D loss: 0.283090] [G loss: 1.682496] [Elapsed time: 239.60s]\n",
      "[Epoch 142/200] [D loss: 0.308584] [G loss: 2.106156] [Elapsed time: 241.31s]\n",
      "[Epoch 143/200] [D loss: 0.293235] [G loss: 1.790489] [Elapsed time: 242.96s]\n",
      "[Epoch 144/200] [D loss: 0.472268] [G loss: 0.883454] [Elapsed time: 244.66s]\n",
      "[Epoch 145/200] [D loss: 0.297011] [G loss: 2.557533] [Elapsed time: 246.36s]\n",
      "[Epoch 146/200] [D loss: 0.281150] [G loss: 2.252533] [Elapsed time: 248.10s]\n",
      "[Epoch 147/200] [D loss: 0.399939] [G loss: 1.694305] [Elapsed time: 249.87s]\n",
      "[Epoch 148/200] [D loss: 0.413468] [G loss: 3.548880] [Elapsed time: 251.60s]\n",
      "[Epoch 149/200] [D loss: 0.344117] [G loss: 1.392657] [Elapsed time: 253.35s]\n",
      "[Epoch 150/200] [D loss: 0.245620] [G loss: 2.606866] [Elapsed time: 255.07s]\n",
      "[Epoch 151/200] [D loss: 0.387335] [G loss: 3.054342] [Elapsed time: 256.70s]\n",
      "[Epoch 152/200] [D loss: 0.219898] [G loss: 1.978679] [Elapsed time: 258.40s]\n",
      "[Epoch 153/200] [D loss: 0.352155] [G loss: 1.287703] [Elapsed time: 260.14s]\n",
      "[Epoch 154/200] [D loss: 0.285361] [G loss: 2.504425] [Elapsed time: 261.86s]\n",
      "[Epoch 155/200] [D loss: 0.281078] [G loss: 2.915916] [Elapsed time: 263.51s]\n",
      "[Epoch 156/200] [D loss: 0.242232] [G loss: 1.727653] [Elapsed time: 265.20s]\n",
      "[Epoch 157/200] [D loss: 0.286392] [G loss: 2.122228] [Elapsed time: 266.86s]\n",
      "[Epoch 158/200] [D loss: 0.288996] [G loss: 2.915232] [Elapsed time: 268.53s]\n",
      "[Epoch 159/200] [D loss: 0.321726] [G loss: 3.374620] [Elapsed time: 270.25s]\n",
      "[Epoch 160/200] [D loss: 0.273753] [G loss: 2.229239] [Elapsed time: 271.96s]\n",
      "[Epoch 161/200] [D loss: 0.175842] [G loss: 2.157656] [Elapsed time: 273.69s]\n",
      "[Epoch 162/200] [D loss: 0.334646] [G loss: 2.676077] [Elapsed time: 275.33s]\n",
      "[Epoch 163/200] [D loss: 0.258707] [G loss: 1.572075] [Elapsed time: 277.02s]\n",
      "[Epoch 164/200] [D loss: 0.223790] [G loss: 2.501233] [Elapsed time: 278.67s]\n",
      "[Epoch 165/200] [D loss: 0.256426] [G loss: 1.802832] [Elapsed time: 280.37s]\n",
      "[Epoch 166/200] [D loss: 0.161440] [G loss: 2.102572] [Elapsed time: 282.11s]\n",
      "[Epoch 167/200] [D loss: 0.228831] [G loss: 1.931721] [Elapsed time: 283.73s]\n",
      "[Epoch 168/200] [D loss: 0.285047] [G loss: 2.523665] [Elapsed time: 285.48s]\n",
      "[Epoch 169/200] [D loss: 0.215878] [G loss: 3.124810] [Elapsed time: 287.27s]\n",
      "[Epoch 170/200] [D loss: 0.253530] [G loss: 2.713574] [Elapsed time: 288.87s]\n",
      "[Epoch 171/200] [D loss: 0.272517] [G loss: 1.490705] [Elapsed time: 290.61s]\n",
      "[Epoch 172/200] [D loss: 0.200328] [G loss: 2.236134] [Elapsed time: 292.23s]\n",
      "[Epoch 173/200] [D loss: 0.445515] [G loss: 4.386927] [Elapsed time: 293.85s]\n",
      "[Epoch 174/200] [D loss: 0.268447] [G loss: 2.075360] [Elapsed time: 295.53s]\n",
      "[Epoch 175/200] [D loss: 0.292041] [G loss: 1.555279] [Elapsed time: 297.18s]\n",
      "[Epoch 176/200] [D loss: 0.418743] [G loss: 3.306077] [Elapsed time: 298.93s]\n",
      "[Epoch 177/200] [D loss: 0.203638] [G loss: 2.332488] [Elapsed time: 300.61s]\n",
      "[Epoch 178/200] [D loss: 0.335685] [G loss: 1.610390] [Elapsed time: 302.30s]\n",
      "[Epoch 179/200] [D loss: 0.237025] [G loss: 3.243768] [Elapsed time: 304.02s]\n",
      "[Epoch 180/200] [D loss: 0.282849] [G loss: 2.239073] [Elapsed time: 305.65s]\n",
      "[Epoch 181/200] [D loss: 0.295362] [G loss: 2.997354] [Elapsed time: 307.26s]\n",
      "[Epoch 182/200] [D loss: 0.233713] [G loss: 2.435907] [Elapsed time: 308.94s]\n",
      "[Epoch 183/200] [D loss: 0.226122] [G loss: 2.097353] [Elapsed time: 310.69s]\n",
      "[Epoch 184/200] [D loss: 0.239848] [G loss: 1.536742] [Elapsed time: 312.44s]\n",
      "[Epoch 185/200] [D loss: 0.186483] [G loss: 2.055267] [Elapsed time: 314.09s]\n",
      "[Epoch 186/200] [D loss: 0.282305] [G loss: 2.444703] [Elapsed time: 315.83s]\n",
      "[Epoch 187/200] [D loss: 0.363407] [G loss: 1.088226] [Elapsed time: 317.56s]\n",
      "[Epoch 188/200] [D loss: 0.246330] [G loss: 2.467737] [Elapsed time: 319.27s]\n",
      "[Epoch 189/200] [D loss: 0.346606] [G loss: 2.456389] [Elapsed time: 320.97s]\n",
      "[Epoch 190/200] [D loss: 0.290449] [G loss: 2.866028] [Elapsed time: 322.62s]\n",
      "[Epoch 191/200] [D loss: 0.229610] [G loss: 2.148252] [Elapsed time: 324.32s]\n",
      "[Epoch 192/200] [D loss: 0.246544] [G loss: 1.895092] [Elapsed time: 326.16s]\n",
      "[Epoch 193/200] [D loss: 0.280669] [G loss: 2.195982] [Elapsed time: 327.90s]\n",
      "[Epoch 194/200] [D loss: 0.178607] [G loss: 1.905575] [Elapsed time: 329.63s]\n",
      "[Epoch 195/200] [D loss: 0.251174] [G loss: 1.660462] [Elapsed time: 331.22s]\n",
      "[Epoch 196/200] [D loss: 0.221865] [G loss: 2.050300] [Elapsed time: 332.90s]\n",
      "[Epoch 197/200] [D loss: 0.326502] [G loss: 1.357973] [Elapsed time: 334.58s]\n",
      "[Epoch 198/200] [D loss: 0.266766] [G loss: 2.774940] [Elapsed time: 336.27s]\n",
      "[Epoch 199/200] [D loss: 0.243160] [G loss: 2.085383] [Elapsed time: 337.99s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 200              # 학습의 횟수(에포크)\n",
    "sample_interval = 2000      # 몇 번의 배치마다 결과 출력할 것인가?\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        # real, fake 이미지에 대한 정답 레이블 생성\n",
    "        real = torch.cuda.FloatTensor(imgs.size(0), 1).fill_(1.0)\n",
    "        fake = torch.cuda.FloatTensor(imgs.size(0), 1).fill_(0.0)\n",
    "\n",
    "        real_imgs = imgs.cuda()\n",
    "\n",
    "        # 생성자(generator)를 학습\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # 랜덤 노이즈(noise) 샘플링\n",
    "        z = torch.normal(mean=0, std=1, size=(imgs.shape[0], latent_dim)).cuda()\n",
    "\n",
    "        # 이미지 생성\n",
    "        generated_imgs = generator(z)\n",
    "\n",
    "        # 생성자(generator)의 손실(loss) 값 계산\n",
    "        g_loss = adversarial_loss(discriminator(generated_imgs), real)\n",
    "\n",
    "        # 생성자(generator) 업데이트\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # 판별자 학습\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # 판별자의 loss 값 계산\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), real)\n",
    "        fake_loss = adversarial_loss(discriminator(generated_imgs.detach()), fake)\n",
    "\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        # 판별자 (discriminator) 업데이트\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        done = epoch * len(dataloader) + i\n",
    "        if done % sample_interval == 0:\n",
    "            save_image(generated_imgs.data[:25], f\"{done}.png\", nrow=5, normalize=True)\n",
    "\n",
    "    # epoch 단위로 log 출력\n",
    "    print(f\"[Epoch {epoch}/{n_epochs}] [D loss: {d_loss.item():.6f}] [G loss: {g_loss.item():.6f}] [Elapsed time: {time.time() - start_time:.2f}s]\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}