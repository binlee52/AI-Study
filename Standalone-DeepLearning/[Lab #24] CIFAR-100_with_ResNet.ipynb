{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab8. CIFAR-100 Classification with ResNet\n",
    "Lab8에서는 assignment4-CIFAR-10 classification with CNN코드의 model architecture 부분을 수정하여 ResNet architecture를 통해 CIFAR-100 classification을 해보겠습니다.\n",
    "\n",
    "ResNet implementation with Pytorch를 참고하였습니다.\n",
    "\n",
    "아래 명령을 통하여 Colab 서버 컴퓨터 내에 결과들을 저장할 results 폴더를 생성합니다. 이미 생성되어 있는 경우 mkdir: cannot create directory 'results': File exists와 같은 에러가 발생하나, 폴더가 이미 존재한다면 상관없으니 넘어가 줍시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `results' 디렉토리를 만들 수 없습니다: 파일이 있습니다\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation\n",
    "기존 CIFAR-10 데이터 저장 코드에서 10을 100으로 바꿔주기만 하면 CIFAR-100 dataset을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "ResNet implementation with Pytorch를 참고하여 ResNet architecture를 구현해봅시다.\n",
    "\n",
    "conv3x3 and conv1x1 functions\n",
    "자주 사용하게 될 1x1과 3x3 filter convolutional layer는 꼭 필요한 parameter인 in_planes, out_planes, stride만을 받아 convolutional layer module를 return해주는 함수를 만들어 사용합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BasicBlock Module\n",
    "2개의 3x3 convolution layer와 skip connection으로 구성된 BasicBlock module을 구현해봅시다.\n",
    "\n",
    "BasicBlock Module Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample    # dimension이 달라졌을 경우 downsample\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # dimension이 변할 때\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck을 지날 때마다 depth를 4배씩 증가시킨다.\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Module\n",
    "적절한 Block type과 layer 수, 그리고 최종적으로 분류할 class 갯수를 받아 ResNet architecture를 구현해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padidng=3, bias=Fasle)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layer[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layer[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layer[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.ini.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "        \n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validate, Test and Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(net, partition, optimizer, criterion, args):\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
    "                                              batch_size=args.train_batch_size, \n",
    "                                              shuffle=True, num_workers=2)\n",
    "    net.train()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다. \n",
    "\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = 100 * correct / total\n",
    "    return net, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, partition, criterion, args):\n",
    "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
    "                                            batch_size=args.test_batch_size, \n",
    "                                            shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(net, partition, args):\n",
    "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
    "                                             batch_size=args.test_batch_size, \n",
    "                                             shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "\n",
    "    net = CNN('VGG19', 3)\n",
    "    net.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "        \n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
    "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
    "        te = time.time()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    test_acc = test(net, partition, args)    \n",
    "    \n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "    return vars(args), result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage Experiment Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import hashlib\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "\n",
    "def save_exp_result(setting, result):\n",
    "    exp_name = setting['exp_name']\n",
    "    del setting['epoch']\n",
    "    del setting['test_batch_size']\n",
    "\n",
    "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
    "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
    "    result.update(setting)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "    \n",
    "def load_exp_result(exp_name):\n",
    "    dir_path = './results'\n",
    "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
    "    list_result = []\n",
    "    for filename in filenames:\n",
    "        if exp_name in filename:\n",
    "            with open(join(dir_path, filename), 'r') as infile:\n",
    "                results = json.load(infile)\n",
    "                list_result.append(results)\n",
    "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(var1, var2, df):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(15, 6)\n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
    "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
    "    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n",
    "    \n",
    "    ax[0].set_title('Train Accuracy')\n",
    "    ax[1].set_title('Validation Accuracy')\n",
    "    ax[2].set_title('Test Accuracy')\n",
    "    \n",
    "def plot_loss_variation(var1, var2, df, **kwargs):\n",
    "\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_losses = list(row.train_losses)[0]\n",
    "            val_losses = list(row.val_losses)[0]\n",
    "\n",
    "            for epoch, train_loss in enumerate(train_losses):\n",
    "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_loss in enumerate(val_losses):\n",
    "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train loss vs Val loss')\n",
    "    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n",
    "    \n",
    "def plot_acc_variation(var1, var2, df, **kwargs):\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_accs = list(row.train_accs)[0]\n",
    "            val_accs = list(row.val_accs)[0]\n",
    "            test_acc = list(row.test_acc)[0]\n",
    "\n",
    "            for epoch, train_acc in enumerate(train_accs):\n",
    "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_acc in enumerate(val_accs):\n",
    "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
    "\n",
    "    def show_acc(x, y, metric, **kwargs):\n",
    "        plt.scatter(x, y, alpha=0.3, s=1)\n",
    "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
    "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
    "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
    "\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
    "    plt.subplots_adjust(top=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret input 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-892ac627e52f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_exp_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exp3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplot_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplot_loss_variation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_acc_variation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargin_titiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-f122711aeb7a>\u001b[0m in \u001b[0;36mplot_acc\u001b[0;34m(var1, var2, df)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"darkgrid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"axes.facecolor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\".9\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mbarplot\u001b[0;34m(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   3167\u001b[0m ):\n\u001b[1;32m   3168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3169\u001b[0;31m     plotter = _BarPlotter(x, y, hue, data, order, hue_order,\n\u001b[0m\u001b[1;32m   3170\u001b[0m                           \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3171\u001b[0m                           \u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1582\u001b[0m                  errwidth, capsize, dodge):\n\u001b[1;32m   1583\u001b[0m         \u001b[0;34m\"\"\"Initialize the plotter.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1584\u001b[0;31m         self.establish_variables(x, y, hue, data, orient,\n\u001b[0m\u001b[1;32m   1585\u001b[0m                                  order, hue_order, units)\n\u001b[1;32m   1586\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mestablish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not interpret input '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# Figure out the plotting orientation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret input 'layers'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var1 = 'layers'\n",
    "var2 = 'lr'\n",
    "\n",
    "df = load_exp_result('exp3')\n",
    "plot_acc(var1, var2, df)\n",
    "plot_loss_variation(var1, var2, df, sharey=False)\n",
    "pl.t_acc_variation(var1, var2, df, margin_titiles=True, sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
